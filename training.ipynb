{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362e7ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import (\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    AdamW,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "#from accelerate import Accelerator\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "from data_process import text_preprocess, build_dataset\n",
    "from config import config\n",
    "import pickle\n",
    "config = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270eea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# accelerator = Accelerator(fp16 = True, cpu = False)\n",
    "# device = accelerator.device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "\n",
    "def get_optim_shedu(named_parameters, total_steps, Hyparameters_config, use_scheduler=True):\n",
    "    ignored_params = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n,p in named_parameters if not any(i in n for i in ignored_params)],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n,p in named_parameters if any(i in n for i in ignored_params)],\n",
    "            \"weight_decay\": 0.0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_parameters, lr=Hyparameters_config['lr'], weight_decay=Hyparameters_config['weight_decay'])\n",
    "    # optimizer = optim.Adam(optimizer_parameters, lr=Hyparameters_config['lr'], weight_decay=Hyparameters_config['weight_decay'])\n",
    "    if use_scheduler:\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps=0.2*total_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#             optimizer,\n",
    "#             mode='max',\n",
    "#             factor=Hyparameters_config['lr_gamma'],\n",
    "#             patience=Hyparameters_config['patience']\n",
    "#         )\n",
    "        return optimizer,scheduler\n",
    "    else:\n",
    "        return optimizer\n",
    "    \n",
    "def trainer(model, train_dataset, valid_dataset, num_epochs, Hyparameters_config):\n",
    "    train_iter = DataLoader(train_dataset, Hyparameters_config['batch_size'], shuffle=True)\n",
    "    valid_iter = DataLoader(valid_dataset, Hyparameters_config['batch_size'], shuffle=False)\n",
    "    total_steps = len(train_iter)*num_epochs\n",
    "    \n",
    "    optimizer,scheduler = get_optim_shedu(model.named_parameters(), total_steps, Hyparameters_config, use_scheduler=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss_train, epoch_loss_eval, epoch_acc_train, epoch_acc_eval = 0, 0, 0, 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print('lr: ',optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        for inputs in tqdm(train_iter, desc=\"training for epoch {}: \".format(epoch+1)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs_1,inputs_2 = {},{}\n",
    "            for name in inputs.keys():\n",
    "                inputs_1[name],inputs_2[name] = torch.chunk(inputs[name].long(), chunks=2, dim=1)\n",
    "                inputs_1[name] = inputs_1[name].contiguous().to(device)\n",
    "                inputs_2[name] = inputs_2[name].contiguous().to(device)\n",
    "                \n",
    "            loss = torch.tensor([0.0], device=device)\n",
    "            outputs = torch.tensor([])\n",
    "            for inputs_ in [inputs_1,inputs_2]:\n",
    "                outputs_dict = model(**inputs_) # outputs: [batch, seq_len, vocab_size]\n",
    "                loss,outputs = loss+outputs_dict['loss'], torch.cat((outputs, outputs_dict['logits'].cpu()), dim=1)\n",
    "            batch_size,seq_len = outputs.size(0), outputs.size(1)\n",
    "            \n",
    "            batch_acc_train = torch.tensor([0.0])\n",
    "            for predicted, target in zip(outputs, inputs['labels'].cpu()):\n",
    "                batch_acc_train += (target == predicted.argmax(dim=1)).sum().item()/seq_len\n",
    "                \n",
    "            #Before = list(model.parameters())[1].clone() # 获取更新前模型的第0层权重\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度截断\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss_train += loss.item()\n",
    "            epoch_acc_train += batch_acc_train.item()/batch_size\n",
    "            #After = list(model.parameters())[1].clone()\n",
    "            #print('encoder的第0层更新幅度：',torch.sum(After-Before))\n",
    "            #print(epoch_loss_train,epoch_acc_train)\n",
    "        \n",
    "            del inputs, inputs_1, inputs_2, outputs_dict, loss, batch_acc_train\n",
    "        \n",
    "        # 参数保存\n",
    "        if (epoch+1) % Hyparameters_config['save_state_epoch']==0:\n",
    "            path = './save_models/'\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            torch.save(\n",
    "                {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                },\n",
    "                path + f'epoch_{epoch+1}.pkl'\n",
    "            )\n",
    "        \n",
    "        # eval\n",
    "        with torch.no_grad():\n",
    "            for inputs in tqdm(valid_iter, desc=\"evaluating for epoch {}: \".format(epoch+1)):\n",
    "                inputs_1,inputs_2 = {},{}\n",
    "                for name in inputs.keys():\n",
    "                    inputs_1[name],inputs_2[name] = torch.chunk(inputs[name].long(), chunks=2, dim=1)\n",
    "                    inputs_1[name] = inputs_1[name].contiguous().to(device)\n",
    "                    inputs_2[name] = inputs_2[name].contiguous().to(device)\n",
    "                \n",
    "                loss = torch.tensor([0.0])\n",
    "                outputs = torch.tensor([])\n",
    "                for inputs_ in [inputs_1,inputs_2]:\n",
    "                    outputs_dict = model(**inputs_) # outputs: [batch, seq_len, vocab_size]\n",
    "                    loss,outputs = loss+outputs_dict['loss'].item(), torch.cat((outputs, outputs_dict['logits'].cpu()), dim=1)\n",
    "                batch_size,seq_len = outputs.size(0), outputs.size(1)\n",
    "                \n",
    "                batch_acc_eval = torch.tensor([0.0])\n",
    "                for predicted,target in zip(outputs, inputs['labels'].cpu()):\n",
    "                    batch_acc_eval += (target == predicted.argmax(dim=1)).sum().item()/seq_len\n",
    "                \n",
    "                epoch_loss_eval += loss.item()\n",
    "                epoch_acc_eval += batch_acc_eval.item()/batch_size\n",
    "        \n",
    "        #scheduler.step(round(epoch_acc_eval/len(valid_iter),2))\n",
    "        del inputs, inputs_1, inputs_2, outputs_dict, loss, batch_acc_eval\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # 参数打印\n",
    "        duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
    "        print(\"Time: {} | Epoch: {}/{} | train_loss: {:.3} | train_acc: {:.3} | eval_loss: {:.3} | eval_acc: {:.3}\".format(\n",
    "            duration, epoch+1, num_epochs, epoch_loss_train/len(train_iter), epoch_acc_train/len(train_iter),\n",
    "            epoch_loss_eval/len(valid_iter), epoch_acc_eval/len(valid_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f1b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(config.train_dataset, 'rb') as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "    with open(config.valid_dataset, 'rb') as f:\n",
    "        valid_dataset = pickle.load(f)  \n",
    "except:\n",
    "    # 文本清洗\n",
    "    train_data = text_preprocess(config.train_path)\n",
    "    valid_data = text_preprocess(config.valid_path)\n",
    "    # 构建dataset\n",
    "    train_dataset = build_dataset(train_data)\n",
    "    valid_dataset = build_dataset(valid_data)\n",
    "    with open(config.train_dataset, 'wb') as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "    with open(config.valid_dataset, 'wb') as f:\n",
    "        pickle.dump(valid_dataset, f)\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5742813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 1: 100%|██████████| 857/857 [10:36<00:00,  1.35it/s]\n",
      "evaluating for epoch 1: 100%|██████████| 63/63 [00:39<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:15 | Epoch: 1/8 | train_loss: 1.57 | train_acc: 0.801 | eval_loss: 3.01 | eval_acc: 0.66\n",
      "lr:  3.125e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 2: 100%|██████████| 857/857 [10:37<00:00,  1.35it/s]\n",
      "evaluating for epoch 2: 100%|██████████| 63/63 [00:34<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:14 | Epoch: 2/8 | train_loss: 1.55 | train_acc: 0.803 | eval_loss: 3.01 | eval_acc: 0.659\n",
      "lr:  4.9519632010080765e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 3: 100%|██████████| 857/857 [10:37<00:00,  1.34it/s]\n",
      "evaluating for epoch 3: 100%|██████████| 63/63 [00:37<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:15 | Epoch: 3/8 | train_loss: 1.53 | train_acc: 0.805 | eval_loss: 3.0 | eval_acc: 0.661\n",
      "lr:  4.432526133406843e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 4: 100%|██████████| 857/857 [10:28<00:00,  1.36it/s]\n",
      "evaluating for epoch 4: 100%|██████████| 63/63 [00:36<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:07 | Epoch: 4/8 | train_loss: 1.52 | train_acc: 0.806 | eval_loss: 3.01 | eval_acc: 0.66\n",
      "lr:  3.4567085809127247e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 5: 100%|██████████| 857/857 [10:25<00:00,  1.37it/s]\n",
      "evaluating for epoch 5: 100%|██████████| 63/63 [00:34<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:00 | Epoch: 5/8 | train_loss: 1.51 | train_acc: 0.807 | eval_loss: 3.01 | eval_acc: 0.66\n",
      "lr:  2.2549571491760985e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 6: 100%|██████████| 857/857 [10:37<00:00,  1.34it/s]\n",
      "evaluating for epoch 6: 100%|██████████| 63/63 [00:37<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:16 | Epoch: 6/8 | train_loss: 1.5 | train_acc: 0.808 | eval_loss: 3.02 | eval_acc: 0.66\n",
      "lr:  1.1110744174509952e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 7: 100%|██████████| 857/857 [10:37<00:00,  1.34it/s]\n",
      "evaluating for epoch 7: 100%|██████████| 63/63 [00:35<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:13 | Epoch: 7/8 | train_loss: 1.49 | train_acc: 0.809 | eval_loss: 3.02 | eval_acc: 0.66\n",
      "lr:  2.9519683912911267e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training for epoch 8: 100%|██████████| 857/857 [10:36<00:00,  1.35it/s]\n",
      "evaluating for epoch 8: 100%|██████████| 63/63 [00:36<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:11:14 | Epoch: 8/8 | train_loss: 1.49 | train_acc: 0.809 | eval_loss: 3.01 | eval_acc: 0.661\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "Hyparameters_config = {\n",
    "    'lr': 5e-6,\n",
    "    'weight_decay': 2e-2,\n",
    "    'batch_size': 8,\n",
    "    'lr_gamma': 0.2,\n",
    "    'patience': 2,\n",
    "    'save_state_epoch': 1\n",
    "}\n",
    "#torch.cuda.empty_cache()\n",
    "saved_model = torch.load(\"save_models/epoch_4.pkl\", map_location=device)\n",
    "model.load_state_dict(saved_model['model_state_dict'])\n",
    "trainer(model, train_dataset, valid_dataset, num_epochs, Hyparameters_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"钛媒体9月23日消息，今日澳门面向内地居民旅游签注全面开放，携程数据显示澳门各类旅游产品搜索量从22日起开始暴增，最高涨幅500%。预计国庆期间，澳门或将迎来旅游小高峰。\"\n",
    "article_input_ids = tokenizer.batch_encode_plus([text], return_tensors='pt', max_length=1024)['input_ids']\n",
    "summary_ids = model.generate(article_input_ids, num_beams=4, length_penalty=2.0, max_length=142, no_repeat_ngram_size=3)\n",
    "summary_txt = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "print(summary_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503d96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
